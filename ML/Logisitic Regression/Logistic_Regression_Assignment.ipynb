{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "hS8akLbxDtrZ"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Theoretical"
      ],
      "metadata": {
        "id": "W8U1Nng6Dhup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1.  What is Logistic Regression, and how does it differ from Linear Regression?\n",
        "\n",
        "Statistical model used for the classification problems.\n",
        "Linear regression predicts continuous values, Logistic regression predicts the probability for a value to fall inside a class.\n",
        "\n",
        "###2. What is the mathematical equation of Logistic Regression?\n",
        "\n",
        "ğœ(ğ‘§)=1/(1+ğ‘’**âˆ’ğ‘§)\n",
        "\n",
        "###3. Why do we use the Sigmoid function in Logistic Regression?\n",
        "\n",
        "Sigmoid function transforms the input to a value between 0 & 1.This is crucial for the classification tasks.\n",
        "\n",
        "Sigmoid function provides\n",
        "Smooth decision boundary\n",
        "Output in probability\n",
        "Thresold based classification\n",
        "\n",
        "###4. What is the cost function of Logistic Regression?\n",
        "ğ½(ğœƒ)=\n",
        "âˆ’1/m ğ‘–=1 to ğ‘š\n",
        " âˆ‘[ğ‘¦(ğ‘–)logâ¡(â„ğœƒ(ğ‘¥(ğ‘–)))+(1âˆ’ğ‘¦(ğ‘–))logâ¡(1âˆ’â„ğœƒ(ğ‘¥ğ‘–)))]\n",
        "\n",
        "\n",
        "ğ½(ğœƒ) - cost function\n",
        "m - number of training examples\n",
        "ğ‘¦(ğ‘–) - actual value\n",
        "â„ğœƒ(ğ‘¥(ğ‘–)) -predicted value using the â„ğœƒ, =1/(1+ğ‘’**âˆ’ğœƒğ‘‡ğ‘¥)\n",
        "\n",
        "###5. What is Regularization in Logistic Regression? Why is it needed?\n",
        "\n",
        "Regularization is used as a penalty in the cost function to reduce overfitting & select the most relevant features\n",
        "\n",
        "+ğœ† ğ‘—=1 to n âˆ‘ âˆ£ğœƒğ‘—âˆ£ (l1/Lasso)\n",
        "\n",
        "ğœ† is penalty strength\n",
        "ğœƒğ‘— is the coefficient\n",
        "\n",
        "\n",
        "###6. Explain the difference between Lasso, Ridge, and Elastic Net regression\n",
        "\n",
        "Lasso - Penalty is ğ‘—=1 to n âˆ‘ âˆ£ğœƒğ‘—âˆ£ , Can bring down the coefficient to zero, helps in feature selection\n",
        "\n",
        "Ridge - Penalty is ğ‘—=1 to n âˆ‘ ğœƒğ‘—**2 , Can not bring down the coefficient to zero, helps in reducing overfitting\n",
        "\n",
        "Elastic net - combination of both lasso & Ridge. penalty is addition of lasso penalty + ridge penalty. Shrinks some coefficients to zero and other to lower value.\n",
        "\n",
        "###7.  When should we use Elastic Net instead of Lasso or Ridge?\n",
        "Highly correlated features - Groups correlated features together\n",
        "Large number of features\n",
        "When feature selection & model generalization both required\n",
        "Balance of Ridge & Lasso can be fine tuned using the alpha parameter\n",
        "alpha = 1 - lasso\n",
        "alpha = 0 Ridge\n",
        "0 < alpha < 1 means mix of both\n",
        "\n",
        "###8. What is the impact of the regularization parameter (Î») in Logistic Regression?\n",
        "\n",
        "Î» determines the strength of regularization or the penalty of regularization. high Î» means more impact of regularization, 0 means no impact\n",
        "\n",
        "\n",
        "###9. What are the key assumptions of Logistic Regression?\n",
        "\n",
        "- No perfect multicollinearity exists\n",
        "- Each observation in the dataset is independent\n",
        "- Outcome is binary for OVR\n",
        "- Large sample size\n",
        "- All predictors have non zero variance\n",
        "- Errors are not having any pattern\n",
        "\n",
        "###10. What are some alternatives to Logistic Regression for classification tasks?\n",
        "\n",
        "- SVM classifier\n",
        "- Naive Bayes classifier\n",
        "- Decision tree clasifier\n",
        "- KNN\n",
        "- Boosting algorithms\n",
        "\n",
        "###11.  What are Classification Evaluation Metrics?\n",
        "\n",
        "- Accuracy\n",
        "- precision\n",
        "- Recall\n",
        "- F1 score\n",
        "- ROC AUC\n",
        "- Confusion matrix\n",
        "\n",
        "###12.  How does class imbalance affect Logistic Regression?\n",
        "\n",
        "- Skewed decision boundary towards majority class\n",
        "- Poor performance for the minority class\n",
        "- Impacts some of the evaluation metrics like accuracy\n",
        "\n",
        "###13. What is Hyperparameter Tuning in Logistic Regression?\n",
        "\n",
        "Hyper parameter tuning is to identify the model parameters which will make the model more accurate and generalised.\n",
        "Some of the parameters are\n",
        "- class weights\n",
        "- solver\n",
        "- regularization\n",
        "\n",
        "###14. What are different solvers in Logistic Regression? Which one should be used?\n",
        "\n",
        "- small dataset - liblinear\n",
        "- elastic net - saga\n",
        "- regularization with lasso/ridge - sag/ saga\n",
        "- multi class clasification - lbfgs\n",
        "\n",
        "###15. How is Logistic Regression extended for multiclass classification\n",
        "\n",
        "Logistic reression can be used as\n",
        "- One vs one - creates binary classification every possible pair of classes\n",
        "- One vs rest - breaks down the multi class problem into multiple binary classification. the class with the highest probability is selected\n",
        "- multnomial - uses softmax function to predict probability of all the classes\n",
        "\n",
        "###16. What are the advantages and disadvantages of Logistic Regression?\n",
        " - Advantages\n",
        "    - Simplicity\n",
        "    - Provides output in probability that can be checked based on a   thresold\n",
        "  -Disadvantage\n",
        "   - Sensitive to outliers\n",
        "   - Assumes linear relationship\n",
        "\n",
        "###17. What are some use cases of Logistic Regression?\n",
        " - Classification in NLP,or customer behaviour\n",
        " - Prediction of continuous varibale in domains like insurance, finance\n",
        "\n",
        "###18. What is the difference between Softmax Regression and Logistic Regression?\n",
        "\n",
        "Logistic regression is good fit for binary classification, Softmax is good fit for multi class classification\n",
        "\n",
        "###19.  How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
        "\n",
        "OVR is useful for small datasets or imbalanced data sets and also for binary classification.\n",
        "\n",
        "Sftmax is suited for multi class classification with huge dtaa size.It takes more computation compared to OVR\n",
        "\n",
        "###20.How do we interpret coefficients in Logistic Regression?\n",
        "\n",
        "Coefficients determines the unit change in the feature impacts the log odds of the prediction class/positive outcome, when other coefficients are not changed.\n"
      ],
      "metadata": {
        "id": "aukmNeIlKC8h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "hS8akLbxDtrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "###1. Write a Python program that loads a dataset, splits it into training and testing sets, applies Logistic\n",
        "###Regression, and prints the model accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z12MIbqCDymo",
        "outputId": "df2e877d-3e7a-43c7-e625-e6252621136d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.95975791 0.04024209]\n",
            "Model accurac is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###2. Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1')\n",
        "###and print the model accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='l1',C=0.7,solver='liblinear')\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW49LQlzOS44",
        "outputId": "80dfb37d-639d-415a-a21e-2d8fed3271c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.05847703 0.94152297]\n",
            "Model accuracy is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###3. Write a Python program to train Logistic Regression with L2 regularization (Ridge) using\n",
        "###LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='l2',C=0.7,solver='lbfgs')\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "print(f\"Model coefficients - {model.coef_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jr5iRfFNQ6r5",
        "outputId": "43e28b28-7f12-457a-bb6c-2a5d8c809dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.9460532 0.0539468]\n",
            "Model accuracy is 1.0\n",
            "Model coefficients - [[ 0.38765039 -0.8267497   2.02656274  0.84650403]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###4. Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='elasticnet',C=0.7,solver='saga',l1_ratio=0.5)\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "print(f\"Model coefficients - {model.coef_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PpBtizR6Sp8C",
        "outputId": "ff094cb9-bfab-4a96-bdeb-eea9ca3328aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.9914192 0.0085808]\n",
            "Model accuracy is 1.0\n",
            "Model coefficients - [[-0.15546817 -1.44485507  2.11568813  0.55202956]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###5. Write a Python program to train a Logistic Regression model for multiclass classification using\n",
        "###multi_class='ovr'\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "print(f\"Unique targets {df['target'].unique()}\")\n",
        "\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(multi_class='ovr',solver='lbfgs')\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "print(f\"Model coefficients - {model.coef_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH096PtAVuNo",
        "outputId": "f9bfce1e-062e-42b1-cc30-838b9fe2802a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets [0 1 2]\n",
            "Model accuracy is 0.9666666666666667\n",
            "Model coefficients - [[-0.41688548  0.83053036 -2.20316668 -0.95388947]\n",
            " [-0.30758509 -1.94643147  0.83786709 -1.4073381 ]\n",
            " [-0.43036227 -0.28995103  2.56978739  2.40384857]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###6. Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic\n",
        "###Regression. Print the best parameters and accuracy\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "print(f\"Unique targets {df['target'].unique()}\")\n",
        "\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_reg = LogisticRegression(multi_class='ovr',solver='lbfgs')\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params ={\n",
        "    'C':[0.5,0.6,0.8,1],\n",
        "    'penalty':['l1','l2','elasticnet','none']\n",
        "\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=logistic_reg,cv=5,param_grid=params,verbose=0,scoring='accuracy')\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_estimator_)\n",
        "print(f\"Best accuracy - {grid_search.best_score_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5uTHzg6amhs",
        "outputId": "f1a1e2da-5520-4e8d-f610-eaa2d29f0740"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets [0 1 2]\n",
            "{'C': 1, 'penalty': 'l2'}\n",
            "LogisticRegression(C=1, multi_class='ovr')\n",
            "Best accuracy - 0.925\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###7. Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the\n",
        "###average accuracy\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "print(f\"Unique targets {df['target'].unique()}\")\n",
        "\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_reg = LogisticRegression(multi_class='ovr',solver='lbfgs')\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "sfk_fold = StratifiedKFold(shuffle=True,n_splits=5)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "acc_score = []\n",
        "for train_index,test_index in sfk_fold.split(X,y):\n",
        "   X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
        "   y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "reg = logistic_reg.fit(X_train,y_train)\n",
        "pred = reg.predict(X_test)\n",
        "accuracy = accuracy_score(pred,y_test)\n",
        "acc_score.append(accuracy)\n",
        "\n",
        "\n",
        "print(f\"Average accuracy {np.mean(acc_score)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZWdRkr6dz1T",
        "outputId": "94b09c2f-35c6-4e29-d798-52fe4c08c885"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets [0 1 2]\n",
            "Average accuracy 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###8. Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its\n",
        "##accuracy\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9M7iTCe0oS18",
        "outputId": "2229d967-3826-437f-d81c-be132e3c7111"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n",
            "\n",
            "Confusion Matrix:\n",
            "[[87 18]\n",
            " [22 52]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.81       105\n",
            "           1       0.74      0.70      0.72        74\n",
            "\n",
            "    accuracy                           0.78       179\n",
            "   macro avg       0.77      0.77      0.77       179\n",
            "weighted avg       0.78      0.78      0.78       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###9. Write a Python program to apply RandomizedSearchCV for tuning hyperparameters (C, penalty, solver) in\n",
        "###Logistic Regression. Print the best parameters and accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "print(f\"Unique targets {df['target'].unique()}\")\n",
        "\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_reg = LogisticRegression(multi_class='ovr')\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "params ={\n",
        "    'C':[0.5,0.6,0.8,1],\n",
        "    'penalty':['l1','l2','elasticnet','none'],\n",
        "    'solver' : ['lbfgs', 'liblinear', 'newton-cg','newton-cholesky', 'sag', 'saga']\n",
        "\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=logistic_reg,cv=5,param_distributions=params,verbose=0,scoring='accuracy')\n",
        "random_search.fit(X_train,y_train)\n",
        "\n",
        "print(random_search.best_params_)\n",
        "print(random_search.best_estimator_)\n",
        "print(f\"Best accuracy - {random_search.best_score_}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oj-laQI6fC7k",
        "outputId": "071c7d81-bebf-4ea6-b9d9-d80a1f4cf945"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets [0 1 2]\n",
            "{'solver': 'sag', 'penalty': 'l2', 'C': 1}\n",
            "LogisticRegression(C=1, multi_class='ovr', solver='sag')\n",
            "Best accuracy - 0.95\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###10.  Write a Python program to implement One-vs-One (OvO) Multiclass Logistic Regression and print accuracy\n",
        "\n",
        "from sklearn.multiclass import OneVsOneClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "data = load_iris()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "# Create a One-vs-One classifier with logistic regression\n",
        "ovo_classifier = OneVsOneClassifier(LogisticRegression(max_iter=200))\n",
        "ovo_classifier.fit(X_train, y_train)\n",
        "\n",
        "y_pred = ovo_classifier.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZvXOJ3rrnrd",
        "outputId": "801dc983-6918-442f-82dc-cf7cd4bbfbfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy 0.9555555555555556\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###11. Write a Python program to train a Logistic Regression model and visualize the confusion matrix for binary\n",
        "##classification\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "report = classification_report(y_pred,y_test)\n",
        "print(f\"confusion matrix is \\n\\n{report}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r7Fdl74MteFd",
        "outputId": "aa9c1f61-a24a-45ac-e774-acd08e17ef7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "confusion matrix is \n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00         9\n",
            "           1       1.00      1.00      1.00        11\n",
            "\n",
            "    accuracy                           1.00        20\n",
            "   macro avg       1.00      1.00      1.00        20\n",
            "weighted avg       1.00      1.00      1.00        20\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###12. Write a Python program to train a Logistic Regression model and evaluate its performance using Precision,\n",
        "###Recall, and F1-Score\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import precision_score,recall_score,f1_score\n",
        "\n",
        "precision = precision_score(y_pred,y_test)\n",
        "print(f\"precision is {precision}\")\n",
        "\n",
        "recall = recall_score(y_pred,y_test)\n",
        "print(f\"recall is {recall}\")\n",
        "\n",
        "f1 = f1_score(y_pred,y_test)\n",
        "print(f\"f1 is {f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Un0x-Ehst_al",
        "outputId": "bc38de87-9d79-40aa-978a-c3a88ca0509c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "precision is 1.0\n",
            "recall is 1.0\n",
            "f1 is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###13 Write a Python program to train a Logistic Regression model on imbalanced data and apply class weights to\n",
        "###improve model performance\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X,y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=3,\n",
        "    n_classes = 2,\n",
        "    weights=[0.1,0.9],\n",
        "    n_redundant=1\n",
        ")\n",
        "\n",
        "noise = np.random.normal(0, 0.5, X.shape)\n",
        "X[y==1]= X[y==1]+noise[y==1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model1 = LogisticRegression()\n",
        "model1.fit(X_train,y_train)\n",
        "\n",
        "y_pred1 = model1.predict(X_test)\n",
        "report1 = classification_report(y_test, y_pred1)\n",
        "print(f\"report before balancing{report1}\")\n",
        "\n",
        "model2 = LogisticRegression(class_weight={0: 0.2, 1: 0.8})\n",
        "model2.fit(X_train,y_train)\n",
        "\n",
        "y_pred2 = model2.predict(X_test)\n",
        "report2 = classification_report(y_test, y_pred2)\n",
        "print(f\"report after balancing{report2}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EwBdXdoMz2a-",
        "outputId": "170b186a-f6c1-444b-ac8c-de8d14ca9e2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "report before balancing              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.50      0.60        28\n",
            "           1       0.95      0.98      0.97       272\n",
            "\n",
            "    accuracy                           0.94       300\n",
            "   macro avg       0.84      0.74      0.78       300\n",
            "weighted avg       0.93      0.94      0.93       300\n",
            "\n",
            "report after balancing              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.25      0.40        28\n",
            "           1       0.93      1.00      0.96       272\n",
            "\n",
            "    accuracy                           0.93       300\n",
            "   macro avg       0.96      0.62      0.68       300\n",
            "weighted avg       0.94      0.93      0.91       300\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###14. Write a Python program to train Logistic Regression on the Titanic dataset, handle missing values, and\n",
        "###evaluate performance\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCBeThkW8aBU",
        "outputId": "82b656ac-4723-4e87-88b0-4b4b54bd31a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n",
            "\n",
            "Confusion Matrix:\n",
            "[[87 18]\n",
            " [22 52]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.81       105\n",
            "           1       0.74      0.70      0.72        74\n",
            "\n",
            "    accuracy                           0.78       179\n",
            "   macro avg       0.77      0.77      0.77       179\n",
            "weighted avg       0.78      0.78      0.78       179\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###15.  Write a Python program to apply feature scaling (Standardization) before training a Logistic Regression\n",
        "###model. Evaluate its accuracy and compare results with and without scaling\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=5, n_classes=2, n_redundant=1\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression()\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07iBe7f59U7-",
        "outputId": "fd38c24c-7f40-4a80-c633-00a3c5cd8bf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.96\n",
            "Accuracy with scaling: 0.96\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###16.  Write a Python program to train Logistic Regression and evaluate its performance using ROC-AUC score\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Predict probabilities for the positive class\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Calculate the ROC AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(f\"ROC AUC Score: {roc_auc:.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n07DnIbO-K_3",
        "outputId": "0d0db170-cca0-4c91-f347-ac8f0f587f7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.78\n",
            "\n",
            "Confusion Matrix:\n",
            "[[87 18]\n",
            " [22 52]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.80      0.83      0.81       105\n",
            "           1       0.74      0.70      0.72        74\n",
            "\n",
            "    accuracy                           0.78       179\n",
            "   macro avg       0.77      0.77      0.77       179\n",
            "weighted avg       0.78      0.78      0.78       179\n",
            "\n",
            "ROC AUC Score: 0.86\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###17.Write a Python program to train Logistic Regression using a custom learning rate (C=0.5) and evaluate\n",
        "###accuracy\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression(penalty='l1',C=0.5,solver='liblinear')\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qF6WNO3-sTX",
        "outputId": "49472782-8690-43e2-a279-5699ee691888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.933327 0.066673]\n",
            "Model accuracy is 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###18. Write a Python program to train Logistic Regression and identify important features based on model\n",
        "###coefficients\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(model['classifier'].coef_)\n",
        "print(f\"With the above result it looks the first coefficient is less\")\n",
        "print(model['classifier'].coef_[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3C7JyDFi-6LP",
        "outputId": "b064778f-3626-47a9-827a-87bb1e2a7fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.09754606  0.52323951  1.15158524 -1.18414262  0.44256203 -0.22988829\n",
            "  -0.24523113]]\n",
            "With the above result it looks the first coefficient is less\n",
            "[-0.09754606  0.52323951  1.15158524 -1.18414262  0.44256203 -0.22988829\n",
            " -0.24523113]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###19. Write a Python program to train Logistic Regression and evaluate its performance using Cohenâ€™s Kappa\n",
        "###Score\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import cohen_kappa_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"cohen_kappa_score {cohen_kappa_score(y_pred,y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t76oC59vCgJP",
        "outputId": "4d22f01a-42d6-4aee-d4b5-f3788fe6e3f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cohen_kappa_score 0.46574252172182795\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###20. Write a Python program to train Logistic Regression and visualize the Precision-Recall Curve for binary\n",
        "###classification\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "\n",
        "from sklearn.metrics import precision_recall_curve, auc\n",
        "\n",
        "y_prob = model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "precision, recall, _ = precision_recall_curve(y_test, y_prob)\n",
        "\n",
        "pr_auc = auc(recall, precision)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(recall, precision, label=f\"Precision-Recall AUC = {pr_auc:.2f}\", color='b')\n",
        "plt.xlabel(\"Recall\")\n",
        "plt.ylabel(\"Precision\")\n",
        "plt.title(\"Precision-Recall Curve\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(alpha=0.3)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 636
        },
        "id": "2zlsm_FDDYdn",
        "outputId": "6422bc8e-9f63-4e50-fec9-7eae7b16750f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.03908628 0.96091372]\n",
            "Model accuracy is 1.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 800x600 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAArwAAAIjCAYAAADhisjVAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAVAJJREFUeJzt3XtcVHX+x/H3DJcBBURjADUUL6m/zLQoWVMzi0QtS7ctU/O2ZWW2tVK5aippGdnFtDKt1tRaS7vnlmmFWWmW5a211LyVd0BLQBAQzvf3h8usI2iAXJzT6/l4zKOZ73zPOd9zPjP59vg9ZxzGGCMAAADAppw1PQAAAACgKhF4AQAAYGsEXgAAANgagRcAAAC2RuAFAACArRF4AQAAYGsEXgAAANgagRcAAAC2RuAFAACArRF4AeAEQ4YMUWxsbLmWWb58uRwOh5YvX14lY/J1V1xxha644grP659//lkOh0Nz586tsTEB+GMh8AKoUXPnzpXD4fA8goKC1KJFC919991KS0ur6eGd9YrDY/HD6XSqXr166tGjh1atWlXTw6sUaWlpuv/++9WqVSvVqlVLtWvXVlxcnB555BEdPny4pocHwAf41/QAAECSJk2apCZNmigvL08rVqzQzJkztXjxYm3cuFG1atWqtnG89NJLsiyrXMtcfvnlOnr0qAIDA6toVL+vX79+6tmzp4qKivTTTz/p+eefV9euXfXtt9+qTZs2NTauM/Xtt9+qZ8+eOnLkiG655RbFxcVJkr777js99thj+uKLL/Txxx/X8CgBnO0IvADOCj169NAll1wiSbrtttt0zjnnaOrUqXr//ffVr1+/UpfJyclR7dq1K3UcAQEB5V7G6XQqKCioUsdRXhdffLFuueUWz+vOnTurR48emjlzpp5//vkaHFnFHT58WH369JGfn5/WrVunVq1aeb0/efJkvfTSS5Wyrar4LAE4ezClAcBZ6corr5Qk7dy5U9LxubUhISHavn27evbsqdDQUA0YMECSZFmWpk2bptatWysoKEhRUVG644479Ntvv5VY70cffaQuXbooNDRUYWFhuvTSS/Xaa6953i9tDu+CBQsUFxfnWaZNmzaaPn265/1TzeF98803FRcXp+DgYEVEROiWW27R3r17vfoU79fevXvVu3dvhYSEyO126/7771dRUVGFj1/nzp0lSdu3b/dqP3z4sP7+978rJiZGLpdLzZs315QpU0qc1bYsS9OnT1ebNm0UFBQkt9ut7t2767vvvvP0mTNnjq688kpFRkbK5XLp/PPP18yZMys85pO98MIL2rt3r6ZOnVoi7EpSVFSUxo0b53ntcDj00EMPlegXGxurIUOGeF4XT6P5/PPPdddddykyMlLnnnuu3nrrLU97aWNxOBzauHGjp23z5s36y1/+onr16ikoKEiXXHKJFi1adGY7DaBKcIYXwFmpOKidc845nrbCwkIlJiaqU6dOevLJJz1THe644w7NnTtXQ4cO1T333KOdO3fqueee07p167Ry5UrPWdu5c+fqr3/9q1q3bq0xY8YoPDxc69at05IlS9S/f/9Sx/HJJ5+oX79+uuqqqzRlyhRJ0qZNm7Ry5Urde++9pxx/8XguvfRSpaSkKC0tTdOnT9fKlSu1bt06hYeHe/oWFRUpMTFR8fHxevLJJ/Xpp5/qqaeeUrNmzTR8+PAKHb+ff/5ZklS3bl1PW25urrp06aK9e/fqjjvuUKNGjfTVV19pzJgx2r9/v6ZNm+bpe+utt2ru3Lnq0aOHbrvtNhUWFurLL7/U119/7TkTP3PmTLVu3VrXXXed/P399e9//1t33XWXLMvSiBEjKjTuEy1atEjBwcH6y1/+csbrKs1dd90lt9utCRMmKCcnR9dcc41CQkL0xhtvqEuXLl59Fy5cqNatW+uCCy6QJP3www/q2LGjGjZsqNGjR6t27dp644031Lt3b7399tvq06dPlYwZQAUZAKhBc+bMMZLMp59+ajIyMszu3bvNggULzDnnnGOCg4PNnj17jDHGDB482Egyo0eP9lr+yy+/NJLM/PnzvdqXLFni1X748GETGhpq4uPjzdGjR736WpbleT548GDTuHFjz+t7773XhIWFmcLCwlPuw2effWYkmc8++8wYY0xBQYGJjIw0F1xwgde2PvjgAyPJTJgwwWt7ksykSZO81nnRRReZuLi4U26z2M6dO40kM3HiRJORkWEOHDhgvvzyS3PppZcaSebNN9/09H344YdN7dq1zU8//eS1jtGjRxs/Pz+za9cuY4wxy5YtM5LMPffcU2J7Jx6r3NzcEu8nJiaapk2berV16dLFdOnSpcSY58yZc9p9q1u3rmnbtu1p+5xIkklOTi7R3rhxYzN48GDP6+LPXKdOnUrUtV+/fiYyMtKrff/+/cbpdHrV6KqrrjJt2rQxeXl5njbLssxll11mzjvvvDKPGUD1YEoDgLNCQkKC3G63YmJidPPNNyskJETvvvuuGjZs6NXv5DOeb775purUqaOrr75aBw8e9Dzi4uIUEhKizz77TNLxM7XZ2dkaPXp0ifm2DofjlOMKDw9XTk6OPvnkkzLvy3fffaf09HTdddddXtu65ppr1KpVK3344Ycllrnzzju9Xnfu3Fk7duwo8zaTk5PldrsVHR2tzp07a9OmTXrqqae8zo6++eab6ty5s+rWret1rBISElRUVKQvvvhCkvT222/L4XAoOTm5xHZOPFbBwcGe55mZmTp48KC6dOmiHTt2KDMzs8xjP5WsrCyFhoae8XpOZdiwYfLz8/Nq69u3r9LT072mp7z11luyLEt9+/aVJP36669atmyZbrrpJmVnZ3uO46FDh5SYmKitW7eWmLoCoGYxpQHAWWHGjBlq0aKF/P39FRUVpZYtW8rp9P47ub+/v84991yvtq1btyozM1ORkZGlrjc9PV3S/6ZIFP+TdFndddddeuONN9SjRw81bNhQ3bp100033aTu3bufcplffvlFktSyZcsS77Vq1UorVqzwaiueI3uiunXres1BzsjI8JrTGxISopCQEM/r22+/XTfeeKPy8vK0bNkyPfPMMyXmAG/dulXff/99iW0VO/FYNWjQQPXq1TvlPkrSypUrlZycrFWrVik3N9frvczMTNWpU+e0y/+esLAwZWdnn9E6TqdJkyYl2rp37646depo4cKFuuqqqyQdn87Qrl07tWjRQpK0bds2GWM0fvx4jR8/vtR1p6enl/jLGoCaQ+AFcFZo3769Z27oqbhcrhIh2LIsRUZGav78+aUuc6pwV1aRkZFav369li5dqo8++kgfffSR5syZo0GDBmnevHlntO5iJ59lLM2ll17qCdLS8TO6J16gdd555ykhIUGSdO2118rPz0+jR49W165dPcfVsixdffXVGjVqVKnbKA50ZbF9+3ZdddVVatWqlaZOnaqYmBgFBgZq8eLFevrpp8t9a7fStGrVSuvXr1dBQcEZ3fLtVBf/nXiGupjL5VLv3r317rvv6vnnn1daWppWrlypRx991NOneN/uv/9+JSYmlrru5s2bV3i8ACofgReAT2vWrJk+/fRTdezYsdQAc2I/Sdq4cWO5w0hgYKB69eqlXr16ybIs3XXXXXrhhRc0fvz4UtfVuHFjSdKWLVs8d5sotmXLFs/75TF//nwdPXrU87pp06an7f/ggw/qpZde0rhx47RkyRJJx4/BkSNHPMH4VJo1a6alS5fq119/PeVZ3n//+9/Kz8/XokWL1KhRI0978RSSytCrVy+tWrVKb7/99ilvTXeiunXrlvghioKCAu3fv79c2+3bt6/mzZun1NRUbdq0ScYYz3QG6X/HPiAg4HePJYCzA3N4Afi0m266SUVFRXr44YdLvFdYWOgJQN26dVNoaKhSUlKUl5fn1c8Yc8r1Hzp0yOu10+nUhRdeKEnKz88vdZlLLrlEkZGRmjVrllefjz76SJs2bdI111xTpn07UceOHZWQkOB5/F7gDQ8P1x133KGlS5dq/fr1ko4fq1WrVmnp0qUl+h8+fFiFhYWSpBtuuEHGGE2cOLFEv+JjVXxW+sRjl5mZqTlz5pR7307lzjvvVP369XXffffpp59+KvF+enq6HnnkEc/rZs2aeeYhF3vxxRfLfXu3hIQE1atXTwsXLtTChQvVvn17r+kPkZGRuuKKK/TCCy+UGqYzMjLKtT0AVY8zvAB8WpcuXXTHHXcoJSVF69evV7du3RQQEKCtW7fqzTff1PTp0/WXv/xFYWFhevrpp3Xbbbfp0ksvVf/+/VW3bl1t2LBBubm5p5yecNttt+nXX3/VlVdeqXPPPVe//PKLnn32WbVr107/93//V+oyAQEBmjJlioYOHaouXbqoX79+ntuSxcbGauTIkVV5SDzuvfdeTZs2TY899pgWLFigBx54QIsWLdK1116rIUOGKC4uTjk5OfrPf/6jt956Sz///LMiIiLUtWtXDRw4UM8884y2bt2q7t27y7Isffnll+ratavuvvtudevWzXPm+4477tCRI0f00ksvKTIystxnVE+lbt26evfdd9WzZ0+1a9fO65fW1q5dq9dff10dOnTw9L/tttt055136oYbbtDVV1+tDRs2aOnSpYqIiCjXdgMCAvTnP/9ZCxYsUE5Ojp588skSfWbMmKFOnTqpTZs2GjZsmJo2baq0tDStWrVKe/bs0YYNG85s5wFUrpq8RQQAFN8i6ttvvz1tv8GDB5vatWuf8v0XX3zRxMXFmeDgYBMaGmratGljRo0aZfbt2+fVb9GiReayyy4zwcHBJiwszLRv3968/vrrXts58bZkb731lunWrZuJjIw0gYGBplGjRuaOO+4w+/fv9/Q5+bZkxRYuXGguuugi43K5TL169cyAAQM8t1n7vf1KTk42ZflfdPEtvp544olS3x8yZIjx8/Mz27ZtM8YYk52dbcaMGWOaN29uAgMDTUREhLnsssvMk08+aQoKCjzLFRYWmieeeMK0atXKBAYGGrfbbXr06GHWrFnjdSwvvPBCExQUZGJjY82UKVPMyy+/bCSZnTt3evpV9LZkxfbt22dGjhxpWrRoYYKCgkytWrVMXFycmTx5ssnMzPT0KyoqMv/4xz9MRESEqVWrlklMTDTbtm075W3JTveZ++STT4wk43A4zO7du0vts337djNo0CATHR1tAgICTMOGDc21115r3nrrrTLtF4Dq4zDmNP+WBwAAAPg45vACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDV+eKIUlmVp3759Cg0NlcPhqOnhAAAA4CTGGGVnZ6tBgwZyOk9/DpfAW4p9+/YpJiampocBAACA37F7926de+65p+1D4C1FaGiopOMHMCwsrMq3Z1mWMjIy5Ha7f/dvKDg7UUPfRw19G/XzfdTQ91V3DbOyshQTE+PJbadD4C1F8TSGsLCwagu8eXl5CgsL40vuo6ih76OGvo36+T5q6PtqqoZlmX7KJwoAAAC2RuAFAACArRF4AQAAYGsEXgAAANgagRcAAAC2RuAFAACArRF4AQAAYGsEXgAAANgagRcAAAC2RuAFAACArRF4AQAAYGsEXgAAANgagRcAAAC2RuAFAACArdVo4P3iiy/Uq1cvNWjQQA6HQ++9997vLrN8+XJdfPHFcrlcat68uebOnVuiz4wZMxQbG6ugoCDFx8dr9erVlT94AAAA+IQaDbw5OTlq27atZsyYUab+O3fu1DXXXKOuXbtq/fr1+vvf/67bbrtNS5cu9fRZuHChkpKSlJycrLVr16pt27ZKTExUenp6Ve0GAAAAzmIOY4yp6UFIksPh0LvvvqvevXufss8//vEPffjhh9q4caOn7eabb9bhw4e1ZMkSSVJ8fLwuvfRSPffcc5Iky7IUExOjv/3tbxo9enSZxpKVlaU6deooMzNTYWFhFd+pMjBGOnLEUkZGhtxut5xOZpn4Isuihr6OGvo26uf7qKHvsyxLR46kKyoqslpqWJ685l/lo6lEq1atUkJCgldbYmKi/v73v0uSCgoKtGbNGo0ZM8bzvtPpVEJCglatWnXK9ebn5ys/P9/zOisrS9LxwlmWVYl7UFJOjhQW5pQUVaXbQVWjhr6PGvo26uf7qKHvc+rSS+tp5cqqzU7FypPRfCrwHjhwQFFR3l+GqKgoZWVl6ejRo/rtt99UVFRUap/Nmzefcr0pKSmaOHFiifaMjAzl5eVVzuBPITfXIb7gAADADr79NlC//LJfISGOKt9WdnZ2mfv6VOCtKmPGjFFSUpLndVZWlmJiYuR2u6tlSsPhw4U6ePCgIiIi+GccH2VZFjX0cdTQt1E/30cNfVtOjlS//vG6ud1uhYZWfQ2DgoLK3NenAm90dLTS0tK82tLS0hQWFqbg4GD5+fnJz8+v1D7R0dGnXK/L5ZLL5SrR7nQ6q+VLFxoqHT0qhYZWz/ZQ+SyLGvo6aujbqJ/vo4a+7cSSVVd+Ks82fOoT1aFDB6Wmpnq1ffLJJ+rQoYMkKTAwUHFxcV59LMtSamqqpw8AAAD+WGo08B45ckTr16/X+vXrJR2/7dj69eu1a9cuScenGgwaNMjT/84779SOHTs0atQobd68Wc8//7zeeOMNjRw50tMnKSlJL730kubNm6dNmzZp+PDhysnJ0dChQ6t13wAAAHB2qNEpDd999526du3qeV08j3bw4MGaO3eu9u/f7wm/ktSkSRN9+OGHGjlypKZPn65zzz1X//znP5WYmOjp07dvX2VkZGjChAk6cOCA2rVrpyVLlpS4kA0AAAB/DGfNfXjPJtV5H17p+LSL9PR0RUZWz33rUPmooe+jhr6N+vk+aujbcnKkkJDjz7OyrGq5aK08eY1PFAAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1mo88M6YMUOxsbEKCgpSfHy8Vq9efcq+x44d06RJk9SsWTMFBQWpbdu2WrJkiVefhx56SA6Hw+vRqlWrqt4NAAAAnKVqNPAuXLhQSUlJSk5O1tq1a9W2bVslJiYqPT291P7jxo3TCy+8oGeffVY//vij7rzzTvXp00fr1q3z6te6dWvt37/f81ixYkV17A4AAADOQjUaeKdOnaphw4Zp6NChOv/88zVr1izVqlVLL7/8cqn9X331VY0dO1Y9e/ZU06ZNNXz4cPXs2VNPPfWUVz9/f39FR0d7HhEREdWxOwAAADgL+dfUhgsKCrRmzRqNGTPG0+Z0OpWQkKBVq1aVukx+fr6CgoK82oKDg0ucwd26dasaNGigoKAgdejQQSkpKWrUqNEpx5Kfn6/8/HzP66ysLEmSZVmyLKvc+1ZelmXJGFMt20LVoIa+jxr6Nurn+6ihbzteNud/n1uqjjKW57NSY4H34MGDKioqUlRUlFd7VFSUNm/eXOoyiYmJmjp1qi6//HI1a9ZMqampeuedd1RUVOTpEx8fr7lz56ply5bav3+/Jk6cqM6dO2vjxo0KDQ0tdb0pKSmaOHFiifaMjAzl5eWdwV6WjWVZyszMlDFGTmeNT6tGBVBD30cNfRv1833U0Lfl5jokHc90GRkZOnrUUeXbzM7OLnPfGgu8FTF9+nQNGzZMrVq1ksPhULNmzTR06FCvKRA9evTwPL/wwgsVHx+vxo0b64033tCtt95a6nrHjBmjpKQkz+usrCzFxMTI7XYrLCys6nbovyzLksPhkNvt5kvuo6ih76OGvo36+T5q6Ntycv733O12KzS06mt48r/6n06NBd6IiAj5+fkpLS3Nqz0tLU3R0dGlLuN2u/Xee+8pLy9Phw4dUoMGDTR69Gg1bdr0lNsJDw9XixYttG3btlP2cblccrlcJdqdTme1fekcDke1bg+Vjxr6Pmro26if76OGvuvEklVXDcuzjRr7RAUGBiouLk6pqameNsuylJqaqg4dOpx22aCgIDVs2FCFhYV6++23df3115+y75EjR7R9+3bVr1+/0sYOAAAA31Gjf4VKSkrSSy+9pHnz5mnTpk0aPny4cnJyNHToUEnSoEGDvC5q++abb/TOO+9ox44d+vLLL9W9e3dZlqVRo0Z5+tx///36/PPP9fPPP+urr75Snz595Ofnp379+lX7/gEAAKDm1egc3r59+yojI0MTJkzQgQMH1K5dOy1ZssRzIduuXbu8Tlfn5eVp3Lhx2rFjh0JCQtSzZ0+9+uqrCg8P9/TZs2eP+vXrp0OHDsntdqtTp076+uuv5Xa7q3v3AAAAcBZwGGNMTQ/ibJOVlaU6deooMzOz2i5aS09PV2RkJPOWfBQ19H3U0LdRP99HDX1bTo4UEnL8eVaWVS0XrZUnr/GJAgAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYWo0H3hkzZig2NlZBQUGKj4/X6tWrT9n32LFjmjRpkpo1a6agoCC1bdtWS5YsOaN1AgAAwN5qNPAuXLhQSUlJSk5O1tq1a9W2bVslJiYqPT291P7jxo3TCy+8oGeffVY//vij7rzzTvXp00fr1q2r8DoBAABgbzUaeKdOnaphw4Zp6NChOv/88zVr1izVqlVLL7/8cqn9X331VY0dO1Y9e/ZU06ZNNXz4cPXs2VNPPfVUhdcJAAAAe/OvqQ0XFBRozZo1GjNmjKfN6XQqISFBq1atKnWZ/Px8BQUFebUFBwdrxYoVFV5n8Xrz8/M9r7OysiRJlmXJsqzy71w5WZYlY0y1bAtVgxr6Pmro26if76OGvu142Zz/fW6pOspYns9KjQXegwcPqqioSFFRUV7tUVFR2rx5c6nLJCYmaurUqbr88svVrFkzpaam6p133lFRUVGF1ylJKSkpmjhxYon2jIwM5eXllXfXys2yLGVmZsoYI6ezxqdVowKooe+jhr6N+vk+aujbcnMdko7nr4yMDB096qjybWZnZ5e5b40F3oqYPn26hg0bplatWsnhcKhZs2YaOnToGU9XGDNmjJKSkjyvs7KyFBMTI7fbrbCwsDMd9u+yLEsOh0Nut5svuY+ihr6PGvo26uf7qKFvy8n533O3263Q0Kqv4cn/6n86NRZ4IyIi5Ofnp7S0NK/2tLQ0RUdHl7qM2+3We++9p7y8PB06dEgNGjTQ6NGj1bRp0wqvU5JcLpdcLleJdqfTWW1fOofDUa3bQ+Wjhr6PGvo26uf7qKHvOrFk1VXD8myjxj5RgYGBiouLU2pqqqfNsiylpqaqQ4cOp102KChIDRs2VGFhod5++21df/31Z7xOAAAA2FONTmlISkrS4MGDdckll6h9+/aaNm2acnJyNHToUEnSoEGD1LBhQ6WkpEiSvvnmG+3du1ft2rXT3r179dBDD8myLI0aNarM6wQAAMAfS40G3r59+yojI0MTJkzQgQMH1K5dOy1ZssRz0dmuXbu8Tlfn5eVp3Lhx2rFjh0JCQtSzZ0+9+uqrCg8PL/M6AQAA8MfiMMaYmh7E2SYrK0t16tRRZmZmtV20lp6ersjISOYt+Shq6PuooW+jfr6PGvq2nBwpJOT486wsq1ouWitPXuMTBQAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbM2/IgsVFRVp7ty5Sk1NVXp6uizL8np/2bJllTI4AAAA4ExVKPDee++9mjt3rq655hpdcMEFcjgclT0uAAAAoFJUKPAuWLBAb7zxhnr27FnZ4wEAAAAqVYXm8AYGBqp58+aVPRYAAACg0lUo8N53332aPn26jDGVPR4AAACgUlVoSsOKFSv02Wef6aOPPlLr1q0VEBDg9f4777xTKYMDAAAAzlSFAm94eLj69OlT2WMBAAAAKl2FAu+cOXMqexwAAABAlahQ4C2WkZGhLVu2SJJatmwpt9tdKYMCAAAAKkuFLlrLycnRX//6V9WvX1+XX365Lr/8cjVo0EC33nqrcnNzK3uMAAAAQIVVKPAmJSXp888/17///W8dPnxYhw8f1vvvv6/PP/9c9913X2WPEQAAAKiwCk1pePvtt/XWW2/piiuu8LT17NlTwcHBuummmzRz5szKGh8AAABwRip0hjc3N1dRUVEl2iMjI5nSAAAAgLNKhQJvhw4dlJycrLy8PE/b0aNHNXHiRHXo0KHSBgcAAACcqQpNaZg+fboSExN17rnnqm3btpKkDRs2KCgoSEuXLq3UAQIAAABnokKB94ILLtDWrVs1f/58bd68WZLUr18/DRgwQMHBwZU6QAAAAOBMVPg+vLVq1dKwYcMqcywAAABApStz4F20aJF69OihgIAALVq06LR9r7vuujMeGAAAAFAZyhx4e/furQMHDigyMlK9e/c+ZT+Hw6GioqLKGBsAAABwxsoceC3LKvU5AAAAcDar0G3JSnP48OHKWhUAAABQaSoUeKdMmaKFCxd6Xt94442qV6+eGjZsqA0bNlTa4AAAAIAzVaHAO2vWLMXExEiSPvnkE3366adasmSJevTooQceeKBSBwgAAACciQoF3gMHDngC7wcffKCbbrpJ3bp106hRo/Ttt9+Wa10zZsxQbGysgoKCFB8fr9WrV5+2/7Rp09SyZUsFBwcrJiZGI0eO9PrFt4ceekgOh8Pr0apVq/LvJAAAAGyhQoG3bt262r17tyRpyZIlSkhIkCQZY8p1h4aFCxcqKSlJycnJWrt2rdq2bavExESlp6eX2v+1117T6NGjlZycrE2bNmn27NlauHChxo4d69WvdevW2r9/v+exYsWKiuwmAAAAbKBCPzzx5z//Wf3799d5552nQ4cOqUePHpKkdevWqXnz5mVez9SpUzVs2DANHTpU0vGpEh9++KFefvlljR49ukT/r776Sh07dlT//v0lSbGxserXr5+++eYb753y91d0dHRFdg0AAAA2U6HA+/TTTys2Nla7d+/W448/rpCQEEnS/v37ddddd5VpHQUFBVqzZo3GjBnjaXM6nUpISNCqVatKXeayyy7Tv/71L61evVrt27fXjh07tHjxYg0cONCr39atW9WgQQMFBQWpQ4cOSklJUaNGjU45lvz8fOXn53teZ2VlSTp++7XquAWbZVkyxnC7Nx9GDX0fNfRt1M/3UUPfdrxszv8+t1QdZSzPZ6VCgTcgIED3339/ifaRI0eWeR0HDx5UUVGRoqKivNqjoqK0efPmUpfp37+/Dh48qE6dOskYo8LCQt15551eUxri4+M1d+5ctWzZUvv379fEiRPVuXNnbdy4UaGhoaWuNyUlRRMnTizRnpGR4TU/uKpYlqXMzEwZY+R0Vtqd4lCNqKHvo4a+jfr5Pmro23JzHZKOZ7qMjAwdPeqo8m1mZ2eXua9P/bTw8uXL9eijj+r5559XfHy8tm3bpnvvvVcPP/ywxo8fL0me6RWSdOGFFyo+Pl6NGzfWG2+8oVtvvbXU9Y4ZM0ZJSUme11lZWYqJiZHb7VZYWFiV7MuJLMuSw+GQ2+3mS+6jqKHvo4a+jfr5Pmro23Jy/vfc7XYrNLTqaxgUFFTmvjX208IRERHy8/NTWlqaV3taWtop59+OHz9eAwcO1G233SZJatOmjXJycnT77bfrwQcfLPULEh4erhYtWmjbtm2nHIvL5ZLL5SrR7nQ6q+1L53A4qnV7qHzU0PdRQ99G/XwfNfRdJ5asumpYnm2UuadlWYqMjPQ8P9WjrHdpCAwMVFxcnFJTU722kZqaqg4dOpS6TG5ubomd8/Pzk3T8DhGlOXLkiLZv36769euXaVwAAACwlwrN4a0sSUlJGjx4sC655BK1b99e06ZNU05OjueuDYMGDVLDhg2VkpIiSerVq5emTp2qiy66yDOlYfz48erVq5cn+N5///3q1auXGjdurH379ik5OVl+fn7q169fje0nAAAAak6FAu8999yj5s2b65577vFqf+6557Rt2zZNmzatTOvp27evMjIyNGHCBB04cEDt2rXTkiVLPBey7dq1y+uM7rhx4+RwODRu3Djt3btXbrdbvXr10uTJkz199uzZo379+unQoUNyu93q1KmTvv76a7nd7orsKgAAAHycw5xqLsBpNGzYUIsWLVJcXJxX+9q1a3Xddddpz549lTbAmpCVlaU6deooMzOz2i5aS09PV2RkJPOWfBQ19H3U0LdRP99HDX1bTo7037vUKivLqpaL1sqT1yo0mkOHDqlOnTol2sPCwnTw4MGKrBIAAACoEhUKvM2bN9eSJUtKtH/00Udq2rTpGQ8KAAAAqCwVmsOblJSku+++WxkZGbryyislSampqXrqqafKPH8XAAAAqA4VCrx//etflZ+fr8mTJ+vhhx+WJMXGxmrmzJkaNGhQpQ4QAAAAOBMVvi3Z8OHDNXz4cGVkZCg4OFghxTOVAQAAgLNIhS+hKyws1Keffqp33nnH86MP+/bt05EjRyptcAAAAMCZqtAZ3l9++UXdu3fXrl27lJ+fr6uvvlqhoaGaMmWK8vPzNWvWrMoeJwAAAFAhFTrDe++99+qSSy7Rb7/9puDgYE97nz59vH4qGAAAAKhpFTrD++WXX+qrr75SYGCgV3tsbKz27t1bKQMDAAAAKkOFzvBalqWioqIS7Xv27FFoaOgZDwoAAACoLBUKvN26dfO6367D4dCRI0eUnJysnj17VtbYAAAAgDNWoSkNTz75pLp3767zzz9feXl56t+/v7Zu3aqIiAi9/vrrlT1GAAAAoMIqFHhjYmK0YcMGLVy4UBs2bNCRI0d06623asCAAV4XsQEAAAA1rdyB99ixY2rVqpU++OADDRgwQAMGDKiKcQEAAACVotxzeAMCApSXl1cVYwEAAAAqXYUuWhsxYoSmTJmiwsLCyh4PAAAAUKkqNIf322+/VWpqqj7++GO1adNGtWvX9nr/nXfeqZTBAQAAAGeqQoE3PDxcN9xwQ2WPBQAAAKh05Qq8lmXpiSee0E8//aSCggJdeeWVeuihh7gzAwAAAM5a5ZrDO3nyZI0dO1YhISFq2LChnnnmGY0YMaKqxgYAAACcsXIF3ldeeUXPP/+8li5dqvfee0///ve/NX/+fFmWVVXjAwAAAM5IuQLvrl27vH46OCEhQQ6HQ/v27av0gQEAAACVoVyBt7CwUEFBQV5tAQEBOnbsWKUOCgAAAKgs5bpozRijIUOGyOVyedry8vJ05513et2ajNuSAQAA4GxRrsA7ePDgEm233HJLpQ0GAAAAqGzlCrxz5sypqnEAAAAAVaJCPy0MAAAA+AoCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsLUaD7wzZsxQbGysgoKCFB8fr9WrV5+2/7Rp09SyZUsFBwcrJiZGI0eOVF5e3hmtEwAAAPZVo4F34cKFSkpKUnJystauXau2bdsqMTFR6enppfZ/7bXXNHr0aCUnJ2vTpk2aPXu2Fi5cqLFjx1Z4nQAAALC3Gg28U6dO1bBhwzR06FCdf/75mjVrlmrVqqWXX3651P5fffWVOnbsqP79+ys2NlbdunVTv379vM7glnedAAAAsDf/mtpwQUGB1qxZozFjxnjanE6nEhIStGrVqlKXueyyy/Svf/1Lq1evVvv27bVjxw4tXrxYAwcOrPA6JSk/P1/5+fme11lZWZIky7JkWdYZ7WdZWJYlY0y1bAtVgxr6Pmro26if76OGvu142Zz/fW6pOspYns9KjQXegwcPqqioSFFRUV7tUVFR2rx5c6nL9O/fXwcPHlSnTp1kjFFhYaHuvPNOz5SGiqxTklJSUjRx4sQS7RkZGSXmB1cFy7KUmZkpY4yczhqfVo0KoIa+jxr6Nurn+6ihb8vNdUg6nr8yMjJ09KijyreZnZ1d5r41FngrYvny5Xr00Uf1/PPPKz4+Xtu2bdO9996rhx9+WOPHj6/weseMGaOkpCTP66ysLMXExMjtdissLKwyhn5almXJ4XDI7XbzJfdR1ND3UUPfRv18HzX0bTk5/3vudrsVGlr1NQwKCipz3xoLvBEREfLz81NaWppXe1pamqKjo0tdZvz48Ro4cKBuu+02SVKbNm2Uk5Oj22+/XQ8++GCF1ilJLpdLLperRLvT6ay2L53D4ajW7aHyUUPfRw19G/XzfdTQd51YsuqqYXm2UWOfqMDAQMXFxSk1NdXTZlmWUlNT1aFDh1KXyc3NLbFzfn5+kiRjTIXWCQAAAHur0SkNSUlJGjx4sC655BK1b99e06ZNU05OjoYOHSpJGjRokBo2bKiUlBRJUq9evTR16lRddNFFnikN48ePV69evTzB9/fWCQAAgD+WGg28ffv2VUZGhiZMmKADBw6oXbt2WrJkieeis127dnmd0R03bpwcDofGjRunvXv3yu12q1evXpo8eXKZ1wkAAIA/FocxxtT0IM42WVlZqlOnjjIzM6vtorX09HRFRkYyb8lHUUPfRw19G/XzfdTQt+XkSCEhx59nZVnVctFaefIanygAAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK0ReAEAAGBrBF4AAADYGoEXAAAAtkbgBQAAgK2dFYF3xowZio2NVVBQkOLj47V69epT9r3iiivkcDhKPK655hpPnyFDhpR4v3v37tWxKwAAADjL+Nf0ABYuXKikpCTNmjVL8fHxmjZtmhITE7VlyxZFRkaW6P/OO++ooKDA8/rQoUNq27atbrzxRq9+3bt315w5czyvXS5X1e0EAAAAzlo1foZ36tSpGjZsmIYOHarzzz9fs2bNUq1atfTyyy+X2r9evXqKjo72PD755BPVqlWrROB1uVxe/erWrVsduwMAAICzTI2e4S0oKNCaNWs0ZswYT5vT6VRCQoJWrVpVpnXMnj1bN998s2rXru3Vvnz5ckVGRqpu3bq68sor9cgjj+icc84pdR35+fnKz8/3vM7KypIkWZYly7LKu1vlZlmWjDHVsi1UDWro+6ihb6N+vo8a+rbjZXP+97ml6ihjeT4rNRp4Dx48qKKiIkVFRXm1R0VFafPmzb+7/OrVq7Vx40bNnj3bq7179+7685//rCZNmmj79u0aO3asevTooVWrVsnPz6/EelJSUjRx4sQS7RkZGcrLyyvnXpWfZVnKzMyUMUZOZ42fdEcFUEPfRw19G/XzfdTQt+XmOiQdz3MZGRk6etRR5dvMzs4uc98an8N7JmbPnq02bdqoffv2Xu0333yz53mbNm104YUXqlmzZlq+fLmuuuqqEusZM2aMkpKSPK+zsrIUExMjt9utsLCwqtuB/7IsSw6HQ263my+5j6KGvo8a+jbq5/uooW/Lyfnfc7fbrdDQqq9hUFBQmfvWaOCNiIiQn5+f0tLSvNrT0tIUHR192mVzcnK0YMECTZo06Xe307RpU0VERGjbtm2lBl6Xy1XqRW1Op7PavnQOh6Nat4fKRw19HzX0bdTP91FD33ViyaqrhuXZRo1+ogIDAxUXF6fU1FRPm2VZSk1NVYcOHU677Jtvvqn8/Hzdcsstv7udPXv26NChQ6pfv/4ZjxkAAAC+pcb/CpWUlKSXXnpJ8+bN06ZNmzR8+HDl5ORo6NChkqRBgwZ5XdRWbPbs2erdu3eJC9GOHDmiBx54QF9//bV+/vlnpaam6vrrr1fz5s2VmJhYLfsEAACAs0eNz+Ht27evMjIyNGHCBB04cEDt2rXTkiVLPBey7dq1q8Qp6y1btmjFihX6+OOPS6zPz89P33//vebNm6fDhw+rQYMG6tatmx5++GHuxQsAAPAH5DDGmJoexNkmKytLderUUWZmZrVdtJaenq7IyEjmLfkoauj7qKFvo36+jxr6tpwcKSTk+POsLKtaLlorT17jEwUAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi8AAAAsDX/mh6ArzLGqLCwUEVFRWe8LsuydOzYMeXl5cnp5O8gvoga+hY/Pz/5+/vL4XDU9FAAANWAwFsBBQUF2r9/v3JzcytlfcYYWZal7Oxs/gD2UdTQ99SqVUv169dXYGBgTQ8FAFDFCLzlZFmWdu7cKT8/PzVo0ECBgYFnHHCKzxZzxsl3UUPfYYxRQUGBMjIytHPnTp133nmclQcAmyPwllNBQYEsy1JMTIxq1apVKeskLPk+auhbgoODFRAQoF9++UUFBQUKCgqq6SEBAKoQpzUqiDNCgG/jOwwAfxz8Hx8AAAC2RuAFAACArRF4UaUcDofee++9Su/r65YvXy6Hw6HDhw9LkubOnavw8PAaHRMAAHZ1VgTeGTNmKDY2VkFBQYqPj9fq1atP2feKK66Qw+Eo8bjmmms8fYwxmjBhgurXr6/g4GAlJCRo69at1bErZ60hQ4Z4jlVgYKCaN2+uSZMmqbCwsEq3u3//fvXo0aPS+56J2NhYz7GoVauW2rRpo3/+859Vvt3Kcscdd8jPz09vvvlmifeGDBmi3r17l2g/OWBLxy/AfPzxx9W2bVvVqlVLERER6tixo+bMmaNjx45Vydjz8vI0ZMgQtWnTRv7+/qWOtTS//vqrBgwYoLCwMIWHh+vWW2/VkSNHvPp8//336ty5s4KCghQTE6PHH3+8CvYAAOCLajzwLly4UElJSUpOTtbatWvVtm1bJSYmKj09vdT+77zzjvbv3+95bNy4UX5+frrxxhs9fR5//HE988wzmjVrlr755hvVrl1biYmJysvLq67dOit1795d+/fv19atW3XffffpoYce0hNPPFFq34KCgkrZZnR0tFwuV6X3PVOTJk3yfH5uueUWDRs2TB999FG1bPtM5ObmasGCBRo1apRefvnlCq+noKBAiYmJeuyxx3T77bfrq6++0urVqzVixAg9++yz+uGHHypx1P9TVFSk4OBg3XPPPUpISCjzcgMGDNAPP/ygTz75RB988IG++OIL3X777Z73s7Ky1K1bNzVu3Fhr1qzRE088oYceekgvvvhiVewGAMDXmBrWvn17M2LECM/roqIi06BBA5OSklKm5Z9++mkTGhpqjhw5YowxxrIsEx0dbZ544glPn8OHDxuXy2Vef/31Mq0zMzPTSDKZmZkl3jt69Kj58ccfzdGjRz1tlmXMkSMVf2RnW+a33wpMdrZVruUsq0y7Y4wxZvDgweb666/3arv66qvNn/70J6/3H3nkEVO/fn0TGxtrjDFm165d5sYbbzR16tQxdevWNdddd53ZuXOn13pmz55tzj//fBMYGGiio6O96inJvPvuu8YYY/Lz882IESNMdHS0cblcplGjRubRRx8tta8xxnz//fema9euJigoyNSrV88MGzbMZGdnl9inJ554wkRHR5t69eqZu+66yxQUFJz2WDRu3Ng8/fTTXm316tUzI0eO9Lz+7bffzK233moiIiJMaGio6dq1q1m/fr3XMosWLTKXXHKJcblc5pxzzjHXXXedsf5blFdeecXExcWZkJAQExUVZfr162fS0tI8y3722WdGkvntt9+MMcbMmTPH1KlT57TjNsaYuXPnmj/96U/m8OHDplatWmbXrl1e75dW59K2N2XKFON0Os3atWtL9C0oKPB8n6rSqcZ6sh9//NFIMt9++62n7aOPPjIOh8Ps3bvXGGPM888/b+rWrWvy8/M9ff7xj3+Yli1bnnK9J3+Xi4qKzP79+01RUVEF9wg1ifr5Pmro244cMUY6/sjKqp4ani6vnaxG78NbUFCgNWvWaMyYMZ42p9OphIQErVq1qkzrmD17tm6++WbVrl1bkrRz504dOHDA6+xRnTp1FB8fr1WrVunmm28usY78/Hzl5+d7XmdlZUk6/iMTlmV59bUsS8YYz0OScnKk0NAzufeqQ1JAuZfKzjb6726XWfGYpeP3Ij106JCnLTU1VWFhYfr4448l/e8s4J/+9Cd98cUX8vf31+TJk9W9e3dt2LBBgYGBmjlzpu677z6lpKSoR48eyszM1MqVK722U3yspk+frkWLFmnhwoVq1KiRdu/erd27d5faNycnR4mJierQoYNWr16t9PR0DRs2THfffbfmzJnj6f/ZZ58pOjpay5Yt07Zt23TzzTerbdu2GjZs2O8eB/PfX0d799139dtvvykgIMAzlhtvvFHBwcFavHix6tSpoxdeeEFXXXWVtmzZonr16unDDz9Unz59NHbsWM2bN08FBQX64IMPPOsuKCjQpEmT1LJlS6Wnp+u+++7TkCFD9OGHH3rV4eTP0onHojSzZ8/2/NN+jx49NGfOHI0fP/60dS5te/Pnz1dCQoLatWtXoq+/v7/8/f1LHcuuXbvUunXr045xzJgxGjt27Gn7nG6sJ/vqq68UHh6uuLg4T9+rrrpKTqdTX3/9tfr06aNVq1bp8ssv96pht27dNGXKFP3666+qW7duqdst/gwUP4pfw/dQP99HDX3b8bI5//vcUnWUsTyflRoNvAcPHlRRUZGioqK82qOiorR58+bfXX716tXauHGjZs+e7Wk7cOCAZx0nr7P4vZOlpKRo4sSJJdozMjJKTIM4duyYLMtSYWGhZ/7r8f+UP7CeqeNjKFvf4j/QCwsLZYzRsmXLtHTpUo0YMUKFhYWyLEu1a9fWzJkzPT+1On/+fBUVFWnWrFmeH1N48cUX5Xa7lZqaqquvvlqTJ0/W3//+d40YMcKzrYsuushrbnBRUZEKCwv1yy+/qHnz5vrTn/4kh8Ohhg0b6k9/+lOpff/1r38pLy9Ps2fPVu3atdWqVStNmzZNffr00SOPPKKoqChZlqW6detq2rRp8vPzU/PmzdWjRw99+umnGjp06GmPx+jRozV+/Hjl5+ersLBQ9erV05AhQ1RYWKiVK1dq9erV2rt3r2eKxWOPPab3339fb7zxhm677TZNnjxZN910kydsGmPUsmVLHTt2TA6HQ4MGDfJsq1GjRpo6dao6dOigw4cPKyQkREVFRSfUsNDzpT3dnOqtW7fq66+/1sKFC1VYWKibb75Zo0aN0ujRoz31ObHOJzp5e1u3btXll19e7jnckZGR+vbbb0/bp169emVa76nGerJ9+/bJ7XaX6FevXj3t27dPhYWF2r9/v2JjY736nHPOOZKkPXv2KDQ0tMR6i4/7oUOHFBAQIMuylJmZKWMM9+j1QdTP91FD35ab65B0PHtlZGTo6NGq/xGm7OzsMvf16V9amz17ttq0aaP27duf0XrGjBmjpKQkz+usrCzFxMTI7XYrLCzMq29eXp6ys7M9Z8EkKSzs+NnWM3Hs2DEFBJQvNNeq5a+y/qiX0+nU4sWLVbduXU9o79+/vyZOnCh/f385nU61adPG69fjNm7cqO3bt6tevXpe68rLy9PPP/+sX3/9Vfv27dPVV1/tORal8fPzk7+/v4YOHapu3brpggsuUGJioq699lp169at1L5btmxR27ZtVadOHc97l19+uSzL0vbt29WwYUM5nU61bt3aa95vgwYNtHHjRvn7++vRRx9VSkqK570ffvhBjRo1kiTdf//9GjJkiPbv369Ro0Zp+PDhatWqlWe/jxw5oujoaK+xHT16VDt37pS/v782bNigYcOGldjv4hquWbNGEydO1IYNG/Tbb795Au2+fft0/vnny8/PT9L/zqYW/8/9dMfxlVdeUWJiomdcvXr10h133KEvvvhCV111laTjdXY6nSXWc/L2jDFyOByn3V5p/P39PcfpTJ1qrKX1O9VYi5cvvgjxxD7Fz0/8rp6o+Lifc845CgoKkmVZcjgccrvd/GHrg6if76OGvs0Y6fDhQh08eFCNGrnl51f1NSzPr2TWaOCNiIiQn5+f0tLSvNrT0tJKhI2T5eTkaMGCBZo0aZJXe/FyaWlpql+/vtc627VrV+q6XC5XqRdLFf+BfHLbiXeHkCSHQwoJOe1wT8sYo8JCh/z9VaU/S9u1a1fPGdwGDRqUCAG1a9f22n5OTo7i4uI0f/78Eus68X9IJx6L0hS/HxcXp507d+qjjz7Sp59+qr59+yohIUFvvfVWib7/O7YOr/dO3l5AQIBXH6fT6fmf5vDhw9W3b1/Pew0bNvT0dbvdOu+883TeeefpzTffVJs2bXTppZfq/PPPV05OjurXr6/ly5eX2Jfw8HA5HA4FBwd7jaM4QErHLyzr3r27EhMTNX/+fLndbu3atUuJiYmeM8An70tp+3uioqIivfLKKzpw4IDXX4yKioo0Z84czxSesLAw/fLLLyXWk5mZKT8/P4WEhMjhcKhFixbasmVLuT9vu3bt0vnnn3/aPmPHji3XlIbfG0P9+vWVnp7u1a+wsFC//vqr6tevL4fDoejo6BJ9ii98Le5T2nYdDofX9/zk1/At1M/3UUPfFhoqHT0q+flVTw3Ls40aDbyBgYGKi4tTamqq5/ZElmUpNTVVd99992mXffPNN5Wfn69bbrnFq71JkyaKjo5WamqqJ+BmZWXpm2++0fDhw6tiN3xG7dq11bx58zL3v/jii7Vw4UJFRkaWONNdLDY2VqmpqeratWuZ1hkWFqa+ffuqb9+++stf/qLu3bvr119/LXEW+f/+7/80d+5c5eTkeOZnr1y5Uk6nUy1btizTturVq1divaWJiYlR3759NWbMGL3//vu6+OKLdeDAAfn7+ys2NrbUZS688EKlpqaWOnVi8+bNOnTokB577DHFxMRIkr777rsyjflUFi9erOzsbK1bt85ztlY6fjZ66NChOnz4sMLDw9WyZUstWLBA+fn5Xn+JW7t2rZo0aeIJy/3799fYsWO1bt06XXTRRV7bOnbsmAoKCjzH/UQNGjTQ+vXrTzvWshzz8iieCrJmzRrFxcVJkpYtWybLshQfH+/p8+CDD3r9S8knn3yili1bljp/FwDwB1PhS+MqyYIFC4zL5TJz5841P/74o7n99ttNeHi4OXDggDHGmIEDB5rRo0eXWK5Tp06mb9++pa7zscceM+Hh4eb9998333//vbn++utNkyZNvO6scDrlvUvDmbIsyxQUFHiu8K8Kv3dFfGnv5+TkmPPOO89cccUV5osvvjA7duwwn332mfnb3/5mdu/ebYw5fteAoKAgM336dPPTTz+ZNWvWmGeeecazDp1w54WnnnrKvPbaa2bTpk1my5Yt5tZbbzXR0dGeK3JP7JuTk2Pq169vbrjhBvOf//zHLFu2zDRt2tQMHjz4tGO+9957TZcuXU57LEq7S8MPP/xgHA6H+fbbb41lWaZTp06mbdu2ZunSpWbnzp1m5cqVZuzYsZ47BXz22WfG6XSaCRMmmB9//NFs2LDBTJ482ViWZdLT001gYKB54IEHzPbt2837779vWrRoYSSZdevWeZZXOe7ScP3115f6eS8qKjLR0dHmueeeM8Ycv7tEZGSkuemmm8x3331ntm7dambPnm1CQ0PNzJkzPcvl5eWZzp07m7p165rnnnvOrF+/3mzfvt0sXLjQXHzxxZ5xVoUffvjBrFu3zvTq1ctcccUVZt26dV7b++abb0zLli3Nnj17PG3du3c3F110kfnmm2/MihUrzHnnnWf69evnef/w4cMmKirKDBw40GzcuNEsWLDA1KpVy7zwwgunHAd3abAX6uf7qKHvq+4alucuDTUeeI0x5tlnnzWNGjUygYGBpn379ubrr7/2vNelSxevkGOMMZs3bzaSzMcff1zq+izLMuPHjzdRUVHG5XKZq666ymzZsqXM4yHw/s/+/fvNoEGDTEREhHG5XKZp06Zm2LBhXsdm1qxZpmXLliYgIMDUr1/f/O1vf/O8d2KIffHFF027du1M7dq1TVhYmLnqqqu8bot1Yl9jyn5bshNVNPAaY0xiYqLp0aOHMcaYrKws87e//c00aNDABAQEmJiYGDNgwACv24C9/fbbpl27diYwMNBERESY3r17e2r42muvmdjYWONyuUyHDh3MokWLKhx4Dxw4YPz9/c0bb7xR6vvDhw83F110kef1li1bTJ8+fUyDBg1M7dq1Tdu2bc1LL71U4vOVl5dnUlJSTJs2bTzHuGPHjmbu3Lnm2LFjpz2GZ6Jx48ZGUolHseJjc+Lt7w4dOmT69etnQkJCTFhYmBk6dKjXZ8EYYzZs2GA6depkXC6XadiwoXnsscdOOw4Cr71QP99HDX3f2Rx4Hcb8zj2B/oCysrJUp04dZWZmlnrR2s6dO9WkSZNyTZY+HWOMCgsLPRffwPdQQ99z8nfZsiylp6crMjKS+YM+iPr5Pmro+6q7hqfLayfjEwUAAABbI/ACAADA1gi8AAAAsDUCLwAAAGyNwFtBXOsH+Da+wwDwx0HgLafim9rn5ubW8EgAnIni73B5f9IbAOB7avSX1nyRn5+fwsPDPT9bWqtWrTO+DRW3tPJ91NB3GGOUm5ur9PR0hYeHe/1yHQDAngi8FRAdHS1JntB7powxsixLTqeTsOSjqKHvCQ8P93yXAQD2RuCtAIfDofr16ysyMlLHjh074/VZlqVDhw7pnHPO4WbbPooa+paAgADO7ALAHwiB9wz4+flVyh+almUpICBAQUFBhCUfRQ0BADh78SczAAAAbI3ACwAAAFsj8AIAAMDWmMNbiuIb0mdlZVXL9izLUnZ2NvM/fRg19H3U0LdRP99HDX1fddewOKeV5YeECLylyM7OliTFxMTU8EgAAABwOtnZ2apTp85p+zgMv69ZgmVZ2rdvn0JDQ6vlnqpZWVmKiYnR7t27FRYWVuXbQ+Wjhr6PGvo26uf7qKHvq+4aGmOUnZ2tBg0a/O4ZZc7wlsLpdOrcc8+t9u2GhYXxJfdx1ND3UUPfRv18HzX0fdVZw987s1uMSTIAAACwNQIvAAAAbI3AexZwuVxKTk6Wy+Wq6aGggqih76OGvo36+T5q6PvO5hpy0RoAAABsjTO8AAAAsDUCLwAAAGyNwAsAAABbI/ACAADA1gi81WTGjBmKjY1VUFCQ4uPjtXr16tP2f/PNN9WqVSsFBQWpTZs2Wrx4cTWNFKdSnhq+9NJL6ty5s+rWrau6desqISHhd2uOqlfe72GxBQsWyOFwqHfv3lU7QJxWeet3+PBhjRgxQvXr15fL5VKLFi34f2kNK28Np02bppYtWyo4OFgxMTEaOXKk8vLyqmm0ONEXX3yhXr16qUGDBnI4HHrvvfd+d5nly5fr4osvlsvlUvPmzTV37twqH+cpGVS5BQsWmMDAQPPyyy+bH374wQwbNsyEh4ebtLS0UvuvXLnS+Pn5mccff9z8+OOPZty4cSYgIMD85z//qeaRo1h5a9i/f38zY8YMs27dOrNp0yYzZMgQU6dOHbNnz55qHjmKlbeGxXbu3GkaNmxoOnfubK6//vrqGSxKKG/98vPzzSWXXGJ69uxpVqxYYXbu3GmWL19u1q9fX80jR7Hy1nD+/PnG5XKZ+fPnm507d5qlS5ea+vXrm5EjR1bzyGGMMYsXLzYPPvigeeedd4wk8+677562/44dO0ytWrVMUlKS+fHHH82zzz5r/Pz8zJIlS6pnwCch8FaD9u3bmxEjRnheFxUVmQYNGpiUlJRS+990003mmmuu8WqLj483d9xxR5WOE6dW3hqerLCw0ISGhpp58+ZV1RDxOypSw8LCQnPZZZeZf/7zn2bw4MEE3hpU3vrNnDnTNG3a1BQUFFTXEPE7ylvDESNGmCuvvNKrLSkpyXTs2LFKx4nfV5bAO2rUKNO6dWuvtr59+5rExMQqHNmpMaWhihUUFGjNmjVKSEjwtDmdTiUkJGjVqlWlLrNq1Sqv/pKUmJh4yv6oWhWp4clyc3N17Ngx1atXr6qGidOoaA0nTZqkyMhI3XrrrdUxTJxCReq3aNEidejQQSNGjFBUVJQuuOACPfrooyoqKqquYeMEFanhZZddpjVr1nimPezYsUOLFy9Wz549q2XMODNnW5bxr5Gt/oEcPHhQRUVFioqK8mqPiorS5s2bS13mwIEDpfY/cOBAlY0Tp1aRGp7sH//4hxo0aFDiy4/qUZEarlixQrNnz9b69eurYYQ4nYrUb8eOHVq2bJkGDBigxYsXa9u2bbrrrrt07NgxJScnV8ewcYKK1LB///46ePCgOnXqJGOMCgsLdeedd2rs2LHVMWScoVNlmaysLB09elTBwcHVOh7O8AJV7LHHHtOCBQv07rvvKigoqKaHgzLIzs7WwIED9dJLLykiIqKmh4MKsCxLkZGRevHFFxUXF6e+ffvqwQcf1KxZs2p6aCij5cuX69FHH9Xzzz+vtWvX6p133tGHH36ohx9+uKaHBh/EGd4qFhERIT8/P6WlpXm1p6WlKTo6utRloqOjy9UfVasiNSz25JNP6rHHHtOnn36qCy+8sCqHidMobw23b9+un3/+Wb169fK0WZYlSfL399eWLVvUrFmzqh00PCryHaxfv74CAgLk5+fnafu///s/HThwQAUFBQoMDKzSMcNbRWo4fvx4DRw4ULfddpskqU2bNsrJydHtt9+uBx98UE4n5+zOZqfKMmFhYdV+dlfiDG+VCwwMVFxcnFJTUz1tlmUpNTVVHTp0KHWZDh06ePWXpE8++eSU/VG1KlJDSXr88cf18MMPa8mSJbrkkkuqY6g4hfLWsFWrVvrPf/6j9evXex7XXXedunbtqvXr1ysmJqY6h/+HV5HvYMeOHbVt2zbPX1Qk6aefflL9+vUJuzWgIjXMzc0tEWqL/wJjjKm6waJSnHVZpkYulfuDWbBggXG5XGbu3Lnmxx9/NLfffrsJDw83Bw4cMMYYM3DgQDN69GhP/5UrVxp/f3/z5JNPmk2bNpnk5GRuS1bDylvDxx57zAQGBpq33nrL7N+/3/PIzs6uqV34wytvDU/GXRpqVnnrt2vXLhMaGmruvvtus2XLFvPBBx+YyMhI88gjj9TULvzhlbeGycnJJjQ01Lz++utmx44d5uOPPzbNmjUzN910U03twh9adna2WbdunVm3bp2RZKZOnWrWrVtnfvnlF2OMMaNHjzYDBw709C++LdkDDzxgNm3aZGbMmMFtyf4Inn32WdOoUSMTGBho2rdvb77++mvPe126dDGDBw/26v/GG2+YFi1amMDAQNO6dWvz4YcfVvOIcbLy1LBx48ZGUolHcnJy9Q8cHuX9Hp6IwFvzylu/r776ysTHxxuXy2WaNm1qJk+ebAoLC6t51DhReWp47Ngx89BDD5lmzZqZoKAgExMTY+666y7z22+/Vf/AYT777LNS/1wrrtngwYNNly5dSizTrl07ExgYaJo2bWrmzJlT7eMu5jCGfxcAAACAfTGHFwAAALZG4AUAAICtEXgBAABgawReAAAA2BqBFwAAALZG4AUAAICtEXgBAABgawReAAAA2BqBFwBwWg6HQ++9954k6eeff5bD4dD69etrdEwAUB4EXgA4iw0ZMkQOh0MOh0MBAQFq0qSJRo0apby8vJoeGgD4DP+aHgAA4PS6d++uOXPm6NixY1qzZo0GDx4sh8OhKVOm1PTQAMAncIYXAM5yLpdL0dHRiomJUe/evZWQkKBPPvlEkmRZllJSUtSkSRMFBwerbdu2euutt7yW/+GHH3TttdcqLCxMoaGh6ty5s7Zv3y5J+vbbb3X11VcrIiJCderUUZcuXbR27dpq30cAqEoEXgDwIRs3btRXX32lwMBASVJKSopeeeUVzZo1Sz/88INGjhypW265RZ9//rkkae/evbr88svlcrm0bNkyrVmzRn/9619VWFgoScrOztbgwYO1YsUKff311zrvvPPUs2dPZWdn19g+AkBlY0oDAJzlPvjgA4WEhKiwsFD5+flyOp167rnnlJ+fr0cffVSffvqpOnToIElq2rSpVqxYoRdeeEFdunTRjBkzVKdOHS1YsEABAQGSpBYtWnjWfeWVV3pt68UXX1R4eLg+//xzXXvttdW3kwBQhQi8AHCW69q1q2bOnKmcnBw9/fTT8vf31w033KAffvhBubm5uvrqq736FxQU6KKLLpIkrV+/Xp07d/aE3ZOlpaVp3LhxWr58udLT01VUVKTc3Fzt2rWryvcLAKoLgRcAznK1a9dW8+bNJUkvv/yy2rZtq9mzZ+uCCy6QJH344Ydq2LCh1zIul0uSFBwcfNp1Dx48WIcOHdL06dPVuHFjuVwudejQQQUFBVWwJwBQMwi8AOBDnE6nxo4dq6SkJP30009yuVzatWuXunTpUmr/Cy+8UPPmzdOxY8dKPcu7cuVKPf/88+rZs6ckaffu3Tp48GCV7gMAVDcuWgMAH3PjjTfKz89PL7zwgu6//36NHDlS8+bN0/bt27V27Vo9++yzmjdvniTp7rvvVlZWlm6++WZ999132rp1q1599VVt2bJFknTeeefp1Vdf1aZNm/TNN99owIABv3tWGAB8DWd4AcDH+Pv76+6779bjjz+unTt3yu12KyUlRTt27FB4eLguvvhijR07VpJ0zjnnaNmyZXrggQfUpUsX+fn5qV27durYsaMkafbs2br99tt18cUXKyYmRo8++qjuv//+mtw9AKh0DmOMqelBAAAAAFWFKQ0AAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFsj8AIAAMDWCLwAAACwNQIvAAAAbI3ACwAAAFv7f3hu7e/sodsyAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###21. Write a Python program to train Logistic Regression with different solvers (liblinear, saga, lbfgs) and compare\n",
        "###their accuracy\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X,y = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=3,\n",
        "    n_classes = 2,\n",
        "    weights=[0.5,0.5],\n",
        "    n_redundant=1\n",
        ")\n",
        "\n",
        "noise = np.random.normal(0, 0.5, X.shape)\n",
        "X[y==1]= X[y==1]+noise[y==1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.4, random_state=42\n",
        ")\n",
        "\n",
        "solver =['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga']\n",
        "\n",
        "for sol in solver:\n",
        "\n",
        "  model = LogisticRegression(solver=sol)\n",
        "  model.fit(X_train,y_train)\n",
        "\n",
        "  y_pred = model.predict(X_test)\n",
        "  from sklearn.metrics import accuracy_score\n",
        "\n",
        "  accuracy = accuracy_score(y_pred,y_test)\n",
        "  print(f\"Model accuracy is {accuracy} with solver {sol}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVRv1HGOEVbv",
        "outputId": "dca5ceab-8c44-4e7c-d7fb-a456dcc00401"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model accuracy is 0.885 with solver lbfgs\n",
            "Model accuracy is 0.885 with solver liblinear\n",
            "Model accuracy is 0.885 with solver newton-cg\n",
            "Model accuracy is 0.885 with solver newton-cholesky\n",
            "Model accuracy is 0.885 with solver sag\n",
            "Model accuracy is 0.885 with solver saga\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###22. Write a Python program to train Logistic Regression and evaluate its performance using Matthews\n",
        "###Correlation Coefficient (MCC)\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "data = pd.read_csv('/content/titanic.csv')\n",
        "\n",
        "features = ['Pclass', 'Sex', 'Age', 'Fare', 'Embarked']\n",
        "target = 'Survived'\n",
        "\n",
        "X = data[features]\n",
        "y = data[target]\n",
        "\n",
        "numerical_features = ['Age', 'Fare']\n",
        "categorical_features = ['Sex', 'Embarked']\n",
        "\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_features),\n",
        "        ('cat', categorical_transformer, categorical_features)\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('classifier', LogisticRegression(max_iter=500, random_state=42))\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(f\"matthews_corrcoef {matthews_corrcoef(y_pred,y_test)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49hKD0uvLIHu",
        "outputId": "80e022b4-b038-440c-b3cc-c67856e6c48c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matthews_corrcoef 0.6284629329038053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###23.  Write a Python program to train Logistic Regression on both raw and standardized data. Compare their\n",
        "###accuracy to see the impact of feature scaling\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X, y = make_classification(\n",
        "    n_samples=1000, n_features=5, n_classes=2, n_redundant=1\n",
        ")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model_no_scaling = LogisticRegression()\n",
        "model_no_scaling.fit(X_train, y_train)\n",
        "y_pred_no_scaling = model_no_scaling.predict(X_test)\n",
        "accuracy_no_scaling = accuracy_score(y_test, y_pred_no_scaling)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_with_scaling = LogisticRegression()\n",
        "model_with_scaling.fit(X_train_scaled, y_train)\n",
        "y_pred_with_scaling = model_with_scaling.predict(X_test_scaled)\n",
        "accuracy_with_scaling = accuracy_score(y_test, y_pred_with_scaling)\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scaling:.2f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_with_scaling:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG1dYXgcM88H",
        "outputId": "ef972189-89a4-42dc-deeb-7ef46ea14875"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.90\n",
            "Accuracy with scaling: 0.90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###24. Write a Python program to train Logistic Regression and find the optimal C (regularization strength) using\n",
        "##cross-validation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "print(f\"Unique targets {df['target'].unique()}\")\n",
        "\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_reg = LogisticRegression(multi_class='ovr',solver='lbfgs')\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "params ={\n",
        "    'C':[0.5,0.6,0.8,1]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(estimator=logistic_reg,cv=5,param_grid=params,verbose=0,scoring='accuracy')\n",
        "grid_search.fit(X_train,y_train)\n",
        "\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_estimator_)\n",
        "print(f\"Best accuracy - {grid_search.best_score_}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "et9vSAgiNJIR",
        "outputId": "a40ea87c-f4c9-4379-981c-154aaf2ab830"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique targets [0 1 2]\n",
            "{'C': 1}\n",
            "LogisticRegression(C=1, multi_class='ovr')\n",
            "Best accuracy - 0.9583333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "###25. Write a Python program to train Logistic Regression, save the trained model using joblib, and load it again to\n",
        "###make predictions.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "df = pd.DataFrame(data.data,columns = data.feature_names)\n",
        "df['target'] = pd.DataFrame(data.target)\n",
        "\n",
        "df = df[df['target']!=2]\n",
        "X =df.iloc[:,:-1]\n",
        "y =df.iloc[:,-1]\n",
        "\n",
        "print(f\"{X.shape} {y.shape}\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2)\n",
        "print(f\"{X_train.shape} {X_test.shape} {y_train.shape} {y_test.shape}\")\n",
        "\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(model.predict_proba(X_test)[5])\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(y_pred,y_test)\n",
        "print(f\"Model accuracy is {accuracy}\")\n",
        "\n",
        "import joblib\n",
        "\n",
        "joblib.dump(model, 'logistic_model.pkl')\n",
        "print(\"Model saved as 'logistic_model.pkl'.\")\n",
        "\n",
        "loaded_model = joblib.load('logistic_model.pkl')\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "loaded_y_pred = loaded_model.predict(X_test)\n",
        "loaded_y_prob = loaded_model.predict_proba(X_test)[5]\n",
        "\n",
        "loaded_accuracy = accuracy_score(y_test, loaded_y_pred)\n",
        "print(f\"Model accuracy after loading: {loaded_accuracy}\")\n",
        "print(f\"Predicted probabilities for the 5th test sample: {loaded_y_prob}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj_tPvYaNX0A",
        "outputId": "48f9c66e-2a92-4978-f0c9-3dc6d8c0275b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(100, 4) (100,)\n",
            "(80, 4) (20, 4) (80,) (20,)\n",
            "[0.0391447 0.9608553]\n",
            "Model accuracy is 1.0\n",
            "Model saved as 'logistic_model.pkl'.\n",
            "Model loaded successfully.\n",
            "Model accuracy after loading: 1.0\n",
            "Predicted probabilities for the 5th test sample: [0.0391447 0.9608553]\n"
          ]
        }
      ]
    }
  ]
}